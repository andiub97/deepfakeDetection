{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T10:28:49.577067Z","iopub.status.busy":"2023-05-24T10:28:49.576362Z","iopub.status.idle":"2023-05-24T10:29:01.332849Z","shell.execute_reply":"2023-05-24T10:29:01.331654Z","shell.execute_reply.started":"2023-05-24T10:28:49.577034Z"},"trusted":true},"outputs":[],"source":["# Load various imports \n","from datetime import datetime\n","from os import listdir\n","from os.path import isfile, join\n","\n","import keras\n","import librosa\n","import librosa.display\n","import librosa.effects\n","\n","\n","import numpy as np\n","import pandas as pd\n","\n","from IPython.display import Audio\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, Model, load_model\n","from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D, Input, Activation\n","#from tensorflow.keras.layers.normalization import BatchNormalization\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras import optimizers\n","\n","from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import soundfile as sf"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-24T10:30:00.555179Z","iopub.status.busy":"2023-05-24T10:30:00.554628Z","iopub.status.idle":"2023-05-24T10:30:00.579845Z","shell.execute_reply":"2023-05-24T10:30:00.578570Z","shell.execute_reply.started":"2023-05-24T10:30:00.555129Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Model\n","\n","def make_attention(x0, l_out_features):\n","    y0 = GlobalAveragePooling2D()(x0)\n","    y0 = Dense(units = l_out_features, input_shape=(y0.shape[1],))(y0)\n","    y0 = Activation('sigmoid')(y0)\n","    y0 = tf.expand_dims(tf.expand_dims(y0, axis=1), axis=1)\n","    x = (x0 * y0) + y0\n","    x = MaxPooling2D(pool_size=(2,2))(x)\n","    return x \n","\n","def resblock1(x0):\n","    model = Conv2D(filters = 20, kernel_size = 3, padding = 'same', strides=(1,1))(x0) \n","    model = BatchNormalization()(model) \n","    model = LeakyReLU(alpha=0.3)(model)\n","    model = Conv2D(filters = 20, kernel_size = 3, padding = 'same', strides=(1,1))(model) \n","    #Downsample kernel 1x1\n","    x1 = Conv2D(filters = 20, kernel_size = 1, padding = 'valid', strides=(1,1))(x0) \n","    model = model + x1\n","    model = MaxPooling2D(pool_size=(2,2))(model)\n","    return model\n","\n","def resblock2(x0): \n","    model = BatchNormalization()(x0) \n","    model = LeakyReLU(alpha=0.3)(model)\n","    model = Conv2D(filters = 64, kernel_size = 3, padding = 'same', strides=(1,1))(model) \n","    model = BatchNormalization()(model) \n","    model = LeakyReLU(alpha=0.3)(model)\n","    model = Conv2D(filters = 64, kernel_size = 3, padding = 'same', strides=(1,1))(model) \n","    #Downsample kernel 1x1\n","    x1 = Conv2D(filters = 64, kernel_size = 1, padding = 'valid', strides=(1,1))(x0)\n","    model = model + x1\n","    model = MaxPooling2D(pool_size=(2,2))(model)\n","    return model\n","\n","def resblock3(x0): \n","    model = BatchNormalization()(x0) \n","    model = LeakyReLU(alpha=0.3)(model)\n","    model = Conv2D(filters = 64, kernel_size = 3, padding = 'same', strides=(1,1))(model) \n","    model = BatchNormalization()(model) \n","    model = LeakyReLU(alpha=0.3)(model)\n","    model = Conv2D(filters = 64, kernel_size = 3, padding = 'same', strides=(1,1))(model) \n","    model = model + x0\n","    model = MaxPooling2D(pool_size=(2,2))(model)\n","    return model\n","\n","def gru_layer(x0):\n","    gru1 = Bidirectional(GRU(64, return_sequences=True))\n","    x = gru1(x0)\n","    gru2 = Bidirectional(GRU(64, return_sequences=False))\n","    x = gru2(x)\n","    return x\n","    \n","def fc(x0):\n","    x0 = Dense(units = 128, input_shape=(x0.shape[1],))(x0)\n","    x0 = Dense(units = 1, input_shape=(x0.shape[1],), use_bias=True)(x0)\n","    x0 = Activation(\"sigmoid\")(x0)\n","    return x0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T10:30:03.523939Z","iopub.status.busy":"2023-05-24T10:30:03.522646Z","iopub.status.idle":"2023-05-24T10:30:03.533750Z","shell.execute_reply":"2023-05-24T10:30:03.532421Z","shell.execute_reply.started":"2023-05-24T10:30:03.523895Z"},"trusted":true},"outputs":[],"source":["from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Input, Permute, Lambda\n","import tensorflow as tf\n","\n","# ORIGINAL\n","num_rows = 80\n","num_columns = 404\n","num_channels = 1\n","\n","def init_model():\n","    inputs = Input(shape = (num_rows, num_columns, num_channels))\n","    bn = BatchNormalization()(inputs) \n","    selu_layer = Activation('selu')(bn)\n","    model = resblock1(selu_layer)\n","    model = make_attention(model,20)\n","    model = resblock2(model)\n","    model = make_attention(model,64)\n","    model = resblock3(model)\n","    model = make_attention(model,64)\n","    bn = BatchNormalization()(model) \n","    selu_layer = Activation('selu')(bn)\n","    x0 = tf.squeeze(selu_layer,axis = 1)\n","    model = gru_layer(x0)\n","    model = fc(model)\n","    # Create model\n","    model = tf.keras.Model(inputs=inputs, outputs=model)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-19T12:29:18.713067Z","iopub.status.busy":"2023-05-19T12:29:18.712198Z","iopub.status.idle":"2023-05-19T12:30:01.076677Z","shell.execute_reply":"2023-05-19T12:30:01.075677Z","shell.execute_reply.started":"2023-05-19T12:29:18.713034Z"},"trusted":true},"outputs":[],"source":["import os, fnmatch, glob\n","global features_list\n","global labels_list\n","\n","\n","features_list=[]\n","labels_list=[]\n","\n","def find(pattern, path, truth):\n","    for root, dirs, files in os.walk(path):\n","        for name in files:\n","            if fnmatch.fnmatch(name, pattern):\n","                features_list.append(os.path.join(root, name))\n","                labels_list.append(truth)\n","\n","find('*.wav', '/kaggle/input/wavefake/generated_audio/','spoof')\n","find('*.wav', '/kaggle/input/ljspeech/','bonafide')\n","find('*.wav', '/kaggle/input/jsut-dataset/','bonafide')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-19T12:30:01.078981Z","iopub.status.busy":"2023-05-19T12:30:01.078642Z","iopub.status.idle":"2023-05-19T12:30:01.104568Z","shell.execute_reply":"2023-05-19T12:30:01.103752Z","shell.execute_reply.started":"2023-05-19T12:30:01.078949Z"},"trusted":true},"outputs":[],"source":["def remove_silence(audio,sr):\n","\n","    buffer = 0.7 * sr\n","    samples_total = len(audio)\n","    samples_wrote = 0\n","    counter = 1\n","    i = 0\n","    arr=[]\n","    \n","    while samples_wrote < samples_total:\n","        #check if the buffer is not exceeding total samples \n","        if buffer > (samples_total - samples_wrote):\n","            buffer = samples_total - samples_wrote\n","        block = audio[int(samples_wrote) : int(samples_wrote + buffer)]\n","        # Write 2 second segment\n","        a,_ = librosa.effects.trim(block, top_db=20)\n","        arr[i:] = a[:]\n","        i=i+len(a)\n","        counter += 1\n","        samples_wrote += buffer\n","        \n","    return np.array(arr)\n","\n","\n","def extract_features(file_name):\n","    max_pad_len = 404\n","    sample_rate = 16000\n","\n","    try:\n","        \n","        audio,sample_rate = librosa.load(file_name, duration=4, sr = sample_rate)        \n","        audio = remove_silence(audio,sample_rate)\n","            \n","        mfcc_audio = librosa.feature.mfcc(y=audio, sr = sample_rate, n_mfcc=80)\n","        pad_width = max_pad_len - mfcc_audio.shape[1]\n","        mfcc_audio = np.pad(mfcc_audio, pad_width=((0, 0), (0, pad_width)), mode='constant')\n","        \n","        return mfcc_audio\n","\n","    except Exception as e:\n","        print(e)\n","        return e\n","    \n","\n","def start_computation(features_list, labels_list):\n","\n","    features_eval=[]\n","    labels_eval=[]\n","\n","    index = 0\n","    data = []\n","    n_mini_audios = 0\n","    \n","    for i in features_list:\n","        data = extract_features(i)\n","\n","        features_eval.append(data)\n","\n","        if labels_list[index] == 'bonafide':\n","            labels_eval.append(0)\n","        else:\n","            labels_eval.append(1)\n","\n","        index = index + 1\n","\n","           \n","    labels_eval=np.array(labels_eval)\n","    features_eval=np.array(features_eval)\n","    features_eval = np.reshape(features_eval, (*features_eval.shape,1))\n","    return features_eval, labels_eval"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-19T12:30:01.107869Z","iopub.status.busy":"2023-05-19T12:30:01.107559Z","iopub.status.idle":"2023-05-19T12:30:01.122495Z","shell.execute_reply":"2023-05-19T12:30:01.121578Z","shell.execute_reply.started":"2023-05-19T12:30:01.107845Z"},"trusted":true},"outputs":[],"source":["class Custom_Generator(keras.utils.Sequence) :\n","  \n","    def __init__(self, features, labels, batch_size) :\n","        self.features = features\n","        self.labels = labels\n","        self.batch_size = batch_size\n","\n","    \n","    def __len__(self):\n","        return (np.ceil(len(self.features) / float(self.batch_size))).astype(int)\n","  \n","\n","    def __getitem__(self, idx) :\n","        batch_x = self.features[idx * self.batch_size : (idx+1) * self.batch_size]\n","        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n","        \n","        batch_features, batch_labels = start_computation(batch_x,batch_y)\n","        \n","        return batch_features,batch_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-19T12:30:01.125027Z","iopub.status.busy":"2023-05-19T12:30:01.124605Z","iopub.status.idle":"2023-05-19T12:30:01.238229Z","shell.execute_reply":"2023-05-19T12:30:01.236752Z","shell.execute_reply.started":"2023-05-19T12:30:01.124995Z"},"trusted":true},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(features_list, labels_list, stratify=labels_list, test_size=0.3, random_state = 42)\n","x_test, x_eval, y_test, y_eval = train_test_split(x_test, y_test, stratify=y_test, test_size=0.5, random_state = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-19T12:30:01.241141Z","iopub.status.busy":"2023-05-19T12:30:01.240332Z","iopub.status.idle":"2023-05-19T12:30:01.246826Z","shell.execute_reply":"2023-05-19T12:30:01.245916Z","shell.execute_reply.started":"2023-05-19T12:30:01.241108Z"},"trusted":true},"outputs":[],"source":["batch_size = 32\n","\n","my_training_batch_generator = Custom_Generator(x_train, y_train, batch_size)\n","my_validation_batch_generator = Custom_Generator(x_test, y_test, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!mkdir model-history"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-19T12:30:38.079859Z","iopub.status.busy":"2023-05-19T12:30:38.079483Z","iopub.status.idle":"2023-05-19T12:30:38.087004Z","shell.execute_reply":"2023-05-19T12:30:38.086161Z","shell.execute_reply.started":"2023-05-19T12:30:38.079829Z"},"trusted":true},"outputs":[],"source":["import csv\n","import tensorflow.keras.backend as K\n","from tensorflow import keras\n","import os\n","\n","model_directory = './model-history/'\n","\n","class StoreModelHistory(keras.callbacks.Callback):\n","\n","    def on_epoch_end(self,batch,logs=None):\n","        if ('lr' not in logs.keys()):\n","            logs.setdefault('lr',0)\n","            logs['lr'] = K.get_value(self.model.optimizer.lr)\n","\n","        if not ('model_history.csv' in os.listdir(model_directory)):\n","            with open(model_directory+'model_history.csv','a') as f:\n","                y=csv.DictWriter(f,logs.keys())\n","                y.writeheader()\n","\n","        with open(model_directory+'model_history.csv','a') as f:\n","            y=csv.DictWriter(f,logs.keys())\n","            y.writerow(logs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-19T12:33:30.482316Z","iopub.status.busy":"2023-05-19T12:33:30.481915Z","iopub.status.idle":"2023-05-19T12:40:17.802681Z","shell.execute_reply":"2023-05-19T12:40:17.797044Z","shell.execute_reply.started":"2023-05-19T12:33:30.482259Z"},"trusted":true},"outputs":[],"source":["import os\n","from keras.callbacks import ReduceLROnPlateau \n","\n","epochs = 10\n","batch_size = 16\n","verbose = 1\n","optimizer = optimizers.Adam(learning_rate = 0.001, beta_1 = 0.9, beta_2 = 0.999)\n","\n","model=init_model()\n","\n","checkpoint_path = './model.ckpt'\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.00001)\n","\n","checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n","                                               monitor='val_loss', \n","                                               save_weights_only=True,\n","                                               verbose=1)\n","\n","es = keras.callbacks.EarlyStopping(\n","        monitor='val_loss', \n","        mode='min', \n","        verbose=1, \n","        patience=2)\n","\n","\n","\n","model.fit(my_training_batch_generator,steps_per_epoch = int( len(x_train) / batch_size),epochs = epochs,verbose = verbose,validation_data = my_validation_batch_generator,validation_steps = int(len(x_test)/ batch_size),callbacks=[checkpoint_callback,reduce_lr,StoreModelHistory(),es])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-19T12:48:51.413776Z","iopub.status.busy":"2023-05-19T12:48:51.413134Z","iopub.status.idle":"2023-05-19T12:48:51.930593Z","shell.execute_reply":"2023-05-19T12:48:51.929706Z","shell.execute_reply.started":"2023-05-19T12:48:51.413742Z"},"trusted":true},"outputs":[],"source":["import csv\n","\n","epochs = 3\n","history_dataframe = pd.read_csv(model_directory+'model_history.csv',sep=',')\n","\n","# Plot training & validation loss values\n","plt.style.use(\"ggplot\")\n","plt.plot(range(1,epochs+1),\n","         history_dataframe['loss'])\n","plt.plot(range(1,epochs+1),\n","         history_dataframe['val_loss'],\n","         linestyle='--')\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","plt.plot(range(1,epochs+1),\n","         history_dataframe['accuracy'])\n","plt.plot(range(1,epochs+1),\n","         history_dataframe['val_accuracy'],\n","         linestyle='--')\n","plt.title('Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-19T09:04:54.091999Z","iopub.status.idle":"2023-05-19T09:04:54.092539Z","shell.execute_reply":"2023-05-19T09:04:54.092216Z","shell.execute_reply.started":"2023-05-19T09:04:54.092197Z"},"trusted":true},"outputs":[],"source":["from keras.models import load_model\n","from keras.utils import plot_model\n","\n","new_model = init_model()\n","new_model.load_weights('/kaggle/input/model-trained/model.ckpt')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-19T09:04:54.095773Z","iopub.status.idle":"2023-05-19T09:04:54.096510Z","shell.execute_reply":"2023-05-19T09:04:54.096301Z","shell.execute_reply.started":"2023-05-19T09:04:54.096280Z"},"trusted":true},"outputs":[],"source":["def extract_features_pred(file_name):\n","    max_pad_len = 200\n","    try:\n","        audio, sample_rate = librosa.load(file_name, duration=4, sr=8000) \n","        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=80)\n","        pad_width = max_pad_len - mfccs.shape[1]\n","        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n","    \n","    except Exception as e:\n","        print(\"Error encountered while parsing file: \" + file_name)\n","        return None\n","\n","    return mfccs\n","\n","def start_computation_pred(features_list, labels_list):\n","\n","    features_dev=[]\n","    labels_dev=[]\n","    \n","    for i in features_list:\n","        data = (extract_features_pred)(i)\n","        features_dev.append(data)\n","        \n","    for j in labels_list:\n","        if j == 'bonafide':        \n","            labels_dev.append(0)\n","        else:\n","            labels_dev.append(1)\n","        \n","    labels_dev=np.array(labels_dev)\n","    features_dev=np.array(features_dev)\n","\n","    features_dev = np.reshape(features_dev, (*features_dev.shape,1))\n","\n","    return features_dev, labels_dev"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-19T09:04:54.098184Z","iopub.status.idle":"2023-05-19T09:04:54.098606Z","shell.execute_reply":"2023-05-19T09:04:54.098431Z","shell.execute_reply.started":"2023-05-19T09:04:54.098389Z"},"trusted":true},"outputs":[],"source":["class My_Custom_Generator(keras.utils.Sequence) :\n","  \n","    def __init__(self, features, labels, batch_size) :\n","        self.features = features\n","        self.labels = labels\n","        self.batch_size = batch_size\n","\n","    \n","    def __len__(self):\n","        return (np.ceil(len(self.features) / float(self.batch_size))).astype(int)\n","  \n","\n","    def __getitem__(self, idx) :\n","        batch_x = self.features[idx * self.batch_size : (idx+1) * self.batch_size]\n","        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n","        \n","        batch_features, batch_labels = start_computation_pred(batch_x,batch_y)\n","        \n","        return batch_features,batch_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-19T09:04:54.100011Z","iopub.status.idle":"2023-05-19T09:04:54.100575Z","shell.execute_reply":"2023-05-19T09:04:54.100225Z","shell.execute_reply.started":"2023-05-19T09:04:54.100206Z"},"trusted":true},"outputs":[],"source":["batch_size = 32\n","my_test_batch_generator = My_Custom_Generator(x_eval, y_eval, batch_size)\n","preds = new_model.predict(x=my_test_batch_generator,steps = int( len(x_eval) / batch_size), verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-19T09:04:54.102149Z","iopub.status.idle":"2023-05-19T09:04:54.102582Z","shell.execute_reply":"2023-05-19T09:04:54.102384Z","shell.execute_reply.started":"2023-05-19T09:04:54.102359Z"},"trusted":true},"outputs":[],"source":["labels_dev=np.array(y_eval[:preds.shape[0]])\n","\n","labels = []\n","for i in labels_dev:\n","    if i == \"bonafide\":\n","        labels.append(0)\n","    else:\n","        labels.append(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-19T09:04:54.103709Z","iopub.status.idle":"2023-05-19T09:04:54.104443Z","shell.execute_reply":"2023-05-19T09:04:54.104242Z","shell.execute_reply.started":"2023-05-19T09:04:54.104220Z"},"trusted":true},"outputs":[],"source":["x = np.rint(preds.flatten()).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-19T09:04:54.106027Z","iopub.status.idle":"2023-05-19T09:04:54.106446Z","shell.execute_reply":"2023-05-19T09:04:54.106257Z","shell.execute_reply.started":"2023-05-19T09:04:54.106239Z"},"trusted":true},"outputs":[],"source":["c_names = ['bonafide', 'spoof']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-19T09:04:54.108040Z","iopub.status.idle":"2023-05-19T09:04:54.108466Z","shell.execute_reply":"2023-05-19T09:04:54.108278Z","shell.execute_reply.started":"2023-05-19T09:04:54.108258Z"},"trusted":true},"outputs":[],"source":["# Classification Report\n","print(classification_report(labels, x, target_names=c_names))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-19T09:04:54.109589Z","iopub.status.idle":"2023-05-19T09:04:54.110388Z","shell.execute_reply":"2023-05-19T09:04:54.110211Z","shell.execute_reply.started":"2023-05-19T09:04:54.110190Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import auc\n","# Calcola i tassi di veri positivi (TPR), i tassi di falsi positivi (FPR) e i threshold\n","fpr, tpr, thresholds = roc_curve(labels, preds, pos_label=1)\n","# Calcola l'area sotto la curva ROC (AUC)\n","roc_auc = auc(fpr, tpr)\n","# Disegna la curva ROC\n","plt.figure()\n","plt.plot(fpr, tpr, label='AUC = %0.2f' % roc_auc)\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-19T09:04:54.111542Z","iopub.status.idle":"2023-05-19T09:04:54.112169Z","shell.execute_reply":"2023-05-19T09:04:54.111986Z","shell.execute_reply.started":"2023-05-19T09:04:54.111966Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import roc_curve, roc_auc_score\n","# supponiamo che y_true sia l'array di etichette di classe true (0 o 1)\n","# e y_pred sia l'array di probabilit√† predette dal modello\n","\n","\n","\n","def calculate_eer(y_true, y_pred):\n","    fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=1)\n","    fnr = 1 - tpr # get FNR , however FPR is same as FAR\n","    eer_threshold = thresholds[np.nanargmin(np.absolute((fnr - fpr)))]\n","    eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n","    return eer\n","\n","\n","eer = calculate_eer(labels, preds)\n","print(\"Mean EER: {:.4f}\".format(eer))\n","\n","# AUC mean\n","# y_true: le vere etichette di classe\n","# y_pred: le etichette di classe predette dal modello\n","# classes: l'elenco delle classi\n","\n","aucs = []\n","for i in range(2):\n","    # Calcola l'AUC per la classe i\n","    auc = roc_auc_score(labels, preds)\n","    aucs.append(auc)\n","\n","# Calcola il Mean AUC\n","mean_auc = np.mean(aucs)\n","print(mean_auc)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
